#!/bin/bash

#SBATCH --job-name=misinfo
#SBATCH --time=08:00:00

# GPU Settings
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --gpus-per-node=1
#SBATCH --partition=gpu
#SBATCH --array=1-1

#SBATCH --output=./job_outputs/misinfo_benchmark_models/llm_inference/%A-%a.out

# Move data to scratch system for fast loading
# Probably not needed for this project
mkdir -p /scratch-local/ivov/misinfo_benchmark_models/data/
LOCAL_DATA_DIR=/scratch-local/ivov/misinfo_benchmark_models/data/
rm -rf LOCAL_DATA_DIR/*
rsync -aru $HOME/misinfo_benchmark_models/data/* $LOCAL_DATA_DIR

# Load environment
module purge
module load 2023
module load Anaconda3/2023.07-2

# Make sure we're in the right directory
cd $HOME/misinfo_benchmark_models/

# Activate environment
source activate misinfo_benchmark

HPARAMS=$HOME/misinfo_benchmark_models/scripts/arrays/year_array.txt

srun python src/llm_inference.py \
    seed=942 \
    year=2017 \
    model_name='meta-llama/Meta-Llama-3-8B-Instruct' \
    data.max_length=512 \
    model.compile=false \
    model.better_transformer=true \
    batch_size=16 \
    acceleration.device_map='auto' \
    acceleration.torch_dtype='fp16' \
    acceleration.low_cpu_mem_usage=true \
    acceleration.data_device='cuda:0' \
    acceleration.max_gpu_memory='40GB' \
    acceleration.max_cpu_memory='120GB' \
    data_dir=$LOCAL_DATA_DIR \
    disable_progress_bar=true \
    $(head -$SLURM_ARRAY_TASK_ID $HPARAMS | tail -1)
